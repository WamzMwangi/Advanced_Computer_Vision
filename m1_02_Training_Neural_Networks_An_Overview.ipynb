{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtdIZy-DauJm"
      },
      "source": [
        "A typical neural network training involves:\n",
        "\n",
        "```\n",
        "Neural Network Training\n",
        "│\n",
        "├── Data\n",
        "│   ├── Dataset Preparation\n",
        "│   └── DataLoader\n",
        "│\n",
        "├── Model Initialization\n",
        "│\n",
        "└── Model Training\n",
        "    ├── Forward Pass\n",
        "    ├── Loss Calculation\n",
        "    ├── Backpropagation\n",
        "    └── Weight Update\n",
        "\n",
        "```\n",
        "\n",
        "<img src=https://learnopencv.com/wp-content/uploads/2024/08/c2-Module01-training-neural-networks-02.png height=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A203BhnCT7E"
      },
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "Before starting the training, it's essential to prepare the dataset properly. The quality of the data directly impacts the model's performance, following the GIGO principle—**Garbage In, Garbage Out**. High-quality, well-curated data ensures that the model can learn meaningful patterns.\n",
        "\n",
        "[Torchvision datasets](https://pytorch.org/vision/stable/datasets.html#datasets) has a set of well structured and readily usable datasets to spin up our training instantly.\n",
        "For eg:\n",
        "- MNIST & FashionMNIST\n",
        "- CIFAR10\n",
        "- ImageNet\n",
        "- Caltech101 etc.,\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "mnist_data = datasets.MNIST(\n",
        "    root=\".\",         # Directory where the dataset will be stored\n",
        "    download=True,    # Download the dataset if it's not already available\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),              # Convert image to tensor\n",
        "        transforms.Normalize((0.5,), (0.5,)) # Normalize with mean and std (for grayscale)\n",
        "    ])\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "Additionally, datasets from Kaggle competitions and institutional data are other sources of quality datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "mnist_data = datasets.MNIST(\n",
        "    root=\".\",         # Directory where the dataset will be stored\n",
        "    download=True,    # Download the dataset if it's not already available\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),              # Convert image to tensor\n",
        "        transforms.Normalize((0.5,), (0.5,)) # Normalize with mean and std (for grayscale)\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz5ezugwOsnb"
      },
      "source": [
        "## DataLoaders\n",
        "\n",
        "Once the dataset is prepared, the next step is to load the data in batches using a [torch.utils.DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#datasets-dataloaders). The DataLoader is a PyTorch utility that efficiently loads data in mini-batches, shuffles the data, and handles other aspects like parallel data loading. This is crucial for training as it ensures the model can process the data into manageable chunks and in a randomized order, which helps in overfitting by reducing the risk of the model learning any sequence patterns in the dataset.\n",
        "\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize DataLoader\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Now 'train_loader' can be used to iterate through the data in mini-batches\n",
        "for batch in train_loader:\n",
        "    inputs, labels = batch\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJf2U5bOSIJJ"
      },
      "source": [
        "# Model Initialization\n",
        "\n",
        "Next, we will move to model preparation. [Torchvision](https://pytorch.org/vision/stable/models.html#models-and-pre-trained-weights), [HuggingFace](https://huggingface.co/models?other=computer-vision) and [PyTorchHub](https://pytorch.org/hub/) provides a range of pre-trained model for various computer vision tasks.\n",
        "\n",
        "These pre-trained models are trained on larger datasets with millions of image samples and classes and can be finetuned for specific datasets, saving computational resources and achieves excellent accuracy.\n",
        "\n",
        "\n",
        "```python\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the final layer to match the number of classes in training dataset\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# (OR)\n",
        "\n",
        "#PyTorchHub\n",
        "model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpNB9UkqUE85"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "Training a neural network involves several steps that are repeated over multiple steps and epochs. During each epoch, the model process the dataset in batches which helps to manage the computational overhead, especially for large datasets.\n",
        "\n",
        "Key Steps in Training:\n",
        "\n",
        "- Forward Pass\n",
        "- Loss Calculation\n",
        "- Backpropagation\n",
        "- Weight Updates\n",
        "\n",
        "\n",
        "Optimizers adjust the model weights during training based on gradients computed in backpropagation. Here's how we define an optimizer.\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the optimizer (e.g., SGD, Adam)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3p5CZ0DVmtO"
      },
      "source": [
        "## Forward Pass\n",
        "\n",
        "During training, over each training step the batch of input images is forward pass through the network layers, to get predictions over entire batch.\n",
        "\n",
        "```python\n",
        "for images, targets in train_loader:\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images) # Model prediction\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETcBipp-X9yT"
      },
      "source": [
        "## Loss Calculation\n",
        "\n",
        "The loss function measures the difference between the predicted outputs and the actual targets or ground truth. This helps to guide the network to improve by minimizing loss.\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Calculate loss (CrossEntropyLoss for classification)\n",
        "loss = F.cross_entropy(outputs, target)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htRZrpR7ZjUC"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "It is the process of computing the gradient of the loss function with respect to each of the model's trainable parameters (`requires_grad = True`). This is done by propagating the loss backward through the network.\n",
        "\n",
        "```python\n",
        "loss.backward()  # Gradient computation\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HmXLs_AZHyR"
      },
      "source": [
        "## Weight Update\n",
        "\n",
        "Finally the optimizer updates the model's weight for that each batch based on computed gradients during backpropagation.\n",
        "\n",
        "\n",
        "**The Weight Update Formula is:**\n",
        "\n",
        "\n",
        "$$ \\mathbf{w}_{\\text{new}} = \\mathbf{w} - \\eta \\nabla L(\\mathbf{w}) $$\n",
        "\n",
        "\n",
        "\n",
        "After the weight update, the gradients are are set to zero to preprare for weight updates of next step.\n",
        "\n",
        "```python\n",
        "# Update weights\n",
        "optimizer.step()\n",
        "\n",
        "# Zero the gradients after updating\n",
        "optimizer.zero_grad()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZABi6ZLUTAh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
